{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1  \n",
    "# Automatically reload bioscout package\n",
    "%aimport bioscout_tech_challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bioscout_tech_challenge import *\n",
    "from bioscout_tech_challenge.imagery import *\n",
    "from bioscout_tech_challenge.models import BoundingBox\n",
    "from bioscout_tech_challenge.utils.file_operations import *\n",
    "from bioscout_tech_challenge.utils import * \n",
    "from bioscout_tech_challenge.utils.image import *\n",
    "# Now any changes to your package will be automatically reloaded\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explore Imagery & Bounding Boxes\n",
    "\n",
    "## Visualise the data\n",
    "\n",
    "To analyse the performance of the machine learning model for detecting diesease within imagery data we need to first provide a mechanism to of transforming the model outputs and ground truths into a comparable format. Let\n",
    "\n",
    "lets start by plotting the ground truth data on top of the image data \n",
    "now lets see if we can plot the model outputs on top of the image data for a visual comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hardcode some data\n",
    "data_folder = \"../data/\"\n",
    "imagery_folder = os.path.join(data_folder, \"imagery/\")\n",
    "model_output_folder = os.path.join(data_folder, \"tables/imagery/model predictions/\")\n",
    "ground_truth_folder = os.path.join(data_folder, \"tables/imagery/ground truths/\")\n",
    "files = os.listdir(imagery_folder)\n",
    "idx = 0\n",
    "test_file = files[idx]\n",
    "file_number = test_file.split(\".\")[0]\n",
    "\n",
    "def display_image(filename):\n",
    "    img_path = os.path.join(imagery_folder, filename)\n",
    "    img = Image.open(img_path)\n",
    "    return img\n",
    "\n",
    "test_image = display_image(test_file)\n",
    "test_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets read in the ground truth data\n",
    "ground_truth_file = os.path.join(ground_truth_folder, f\"{file_number}.csv\")\n",
    "ground_truth_df = read_csv_file(ground_truth_file)\n",
    "display(ground_truth_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good lets plot the ground truth data on top of the image data. The ground truth data stores diesease as a bounding box with the top left and bottom right coordinates as pixel values. Lets plot the bounding boxes on top of the image data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw = ImageDraw.Draw(test_image)   \n",
    "for idx, row in ground_truth_df.iterrows():\n",
    "    draw.rectangle([(row[\"x_min\"], row[\"y_min\"]), (row[\"x_max\"], row[\"y_max\"])], outline=\"red\", width=3)\n",
    "test_image.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks great we need to develop this into the package for useablity when analysing.\n",
    "\n",
    "Now lets plot the model outputs on top of the image data for a visual comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the model outputs\n",
    "# Search for file with any extension matching the number\n",
    "matching_files = [f for f in os.listdir(model_output_folder) if f.startswith(f\"{file_number}.\")]\n",
    "if matching_files:\n",
    "    model_output_file = os.path.join(model_output_folder, matching_files[0])\n",
    "else:\n",
    "    raise FileNotFoundError(f\"No file found starting with {file_number} in {model_output_folder}\")\n",
    "model_output_df = read_csv_file(model_output_file)\n",
    "model_output_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load this into a bounding box object\n",
    "from bioscout_tech_challenge.models.bounding_box import BoundingBox\n",
    "\n",
    "for idx, row in model_output_df.iterrows():\n",
    "    box = BoundingBox.from_centroid(row[\"x_center_normalised\"], row[\"y_center_normalised\"], row[\"width_normalised\"], row[\"height_normalised\"])\n",
    "    draw.rectangle(box.to_absolute_coordinates(test_image.width, test_image.height), outline=\"blue\", width=3)\n",
    "    \n",
    "test_image.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate IoU\n",
    "Looks good we have a visual comparison of the model outputs and ground truths. Now we need a way to compare the model outputs and ground truths. Commonly IoU (Intersection over Union) is used to compare the model outputs and ground truths. This measures the overlap normalized by the union of the two bounding boxes giving a value between 0 and 1. Typically a threshold of 0.5 is used to determine if a prediction is a true positive. \n",
    "\n",
    "Lets calculate the IoU for the boxes identified above and show the results on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_text_with_outline(draw, box, image_width, image_height, text, font_size=72):\n",
    "    \"\"\"\n",
    "    Draw text centered in the bounding box with a black outline for better visibility.\n",
    "    \"\"\"\n",
    "    # Get box coordinates\n",
    "    x1, y1, x2, y2 = box.to_absolute_coordinates(image_width, image_height)\n",
    "    \n",
    "    # Calculate center of box\n",
    "    center_x = (x1 + x2) // 2\n",
    "    center_y = (y1 + y2) // 2\n",
    "    \n",
    "    # Get text size using default font\n",
    "    font = ImageFont.load_default().font_variant(size=font_size)\n",
    "    text_width = draw.textlength(text, font=font)  # Get width of text\n",
    "    text_height = font_size  # Approximate height for default font\n",
    "    \n",
    "    # Calculate text position (centered)\n",
    "    text_x = center_x - text_width // 2\n",
    "    text_y = center_y - text_height // 2\n",
    "    \n",
    "    # Draw outline\n",
    "    offset = 2\n",
    "    for dx, dy in [(-offset,-offset), (-offset,offset), (offset,-offset), (offset,offset)]:\n",
    "        draw.text((text_x+dx, text_y+dy), text, fill=\"black\")\n",
    "    \n",
    "    # Draw main text\n",
    "    draw.text((text_x, text_y), text, fill=\"white\",font=font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of bounding boxes from the model outputs\n",
    "model_output_boxes = [BoundingBox.from_centroid(row[\"x_center_normalised\"], row[\"y_center_normalised\"], row[\"width_normalised\"], row[\"height_normalised\"]) for idx, row in model_output_df.iterrows()]\n",
    "from PIL import ImageFont\n",
    "\n",
    "\n",
    "# Create a list of bounding boxes from the ground truth data\n",
    "ground_truth_boxes = [BoundingBox.from_absolute_coordinates(\\\n",
    "    row[\"x_min\"], \\\n",
    "    row[\"y_min\"], \\\n",
    "    row[\"x_max\"], \\\n",
    "    row[\"y_max\"], \\\n",
    "    test_image.width, \\\n",
    "    test_image.height) for idx, row in ground_truth_df.iterrows()]\n",
    "\n",
    "matches = find_box_matches(model_output_boxes, ground_truth_boxes)\n",
    "\n",
    "for pred_idx, gt_idx in matches.items():\n",
    "    iou = model_output_boxes[pred_idx].calculate_iou(ground_truth_boxes[gt_idx])\n",
    "    centroid = model_output_boxes[pred_idx].to_absolute_centroid(test_image.width, test_image.height)\n",
    "    # Use the font when drawing text\n",
    "\n",
    "    draw_text_with_outline(draw, model_output_boxes[pred_idx], test_image.width, test_image.height, f\"{iou:.2f}\")\n",
    "test_image.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great we can visualise the IoU scores on the image data. There is one issue here, only a single IoU score is displayed for the two sets of boxes that overlap at the bottom centre of the image. This actually seems to be both an issue with the ground truth data labelling and the model outputs. The ground truth data has labelled to of three overlapping diesease areas as a single bounding box. Whilst the model outputs have successfully seperated one these out from the single box, however it has also incorrectly identified all three diesease areas as a single bounding box. \n",
    "\n",
    "This is a complicated issue and as such will be left for now. Possible solutions include:\n",
    "\n",
    "1. Post processing the model outputs to merge the boxes that are too close to each other.\n",
    "2. Post processing the ground truth data to merge the boxes that are too close to each other.\n",
    "3. Using a more complex model that can identify multiple diesease areas within a single bounding box.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate some metrics for the a single image.\n",
    "Lets calculate some metrics for the single test image so far. We can use the IoU scores to determine the number of true positives and false positives. We can then use the true positives to calculate the precision, recall and F1 score. These are common metrics used in evaulating the performance of a machine learning model. Whilst these metrics are simple to calculate it was first important to establish a way to compare the model outputs and ground truths using the IoU scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of true positives\n",
    "true_positives, matched_gt_boxes = find_true_positives(model_output_boxes, ground_truth_boxes,iou_threshold=0.5)\n",
    "print(f\"True Positives: {true_positives}\")\n",
    "    \n",
    "# Calculate the number of false positives\n",
    "false_positives = len(model_output_boxes) - true_positives\n",
    "print(f\"False Positives: {false_positives}\")\n",
    "\n",
    "# Calculate the number of false negatives\n",
    "false_negatives = len(ground_truth_boxes) - true_positives\n",
    "print(f\"False Negatives: {false_negatives}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matches the visualisation of the IoU scores on the image data. We have one false positive and negative from the overlap of the two sets of boxes. Whilst an additional false positive that appears to be correct however, it was not identified by the ground truth data. There are three additional false negatives that were not identified by the model outputs. However, the final one appears to be a true negative as there is a labelled ground truth box that runs off the image and cannot be identified as a diesease area.\n",
    "\n",
    "Overall just this one image has given us a good indication of the performance of the model and has raised some issues with the data labelling.\n",
    "\n",
    "Although this is a small sample lets just test the precision, recall and F1 score calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "precision = calculate_precision(true_positives, len(model_output_boxes))\n",
    "recall = calculate_recall(true_positives, len(ground_truth_boxes))\n",
    "f1 = calculate_f1(precision, recall)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a good result. The precision, recall and F1 score are all relatively high. However, this is only a single image and the metrics are not very robust. We need to calculate the metrics for all the images to get a better indication of the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe to store metrics of all images\n",
    "We will create a dataframe to store and keep track of the metrics for the images as we process them. THis will allow us to easily calculate the metrics per image and across the dataset. As well as allow us to add functionality like calculating the average precision across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets read in all the model outputs and ground truths and return a dataframe combining the data.\n",
    "# We can reuse our functions developed in utils.file_operations\n",
    "import bioscout_tech_challenge.utils.image as image_utils\n",
    "ground_truth_files = find_csv_files(ground_truth_folder,sort=True)\n",
    "model_output_files = find_csv_files(model_output_folder,sort=True)\n",
    "\n",
    "ground_truth_df = combine_csv_files(ground_truth_files,detect_header=False)\n",
    "model_output_df = combine_csv_files(model_output_files,detect_header=False)\n",
    "\n",
    "# since we automatically tag the source file we just need to remove the file type to get the file number\n",
    "ground_truth_df[\"file_number\"] = ground_truth_df[\"source_file\"].str.replace(\".csv\", \"\")\n",
    "model_output_df[\"file_number\"] = model_output_df[\"source_file\"].str.replace(\".jpg.csv\", \"\")\n",
    "\n",
    "display(ground_truth_df.head())\n",
    "display(model_output_df.head())\n",
    "\n",
    "image_files = image_utils.find_image_files(imagery_folder,sort=True)\n",
    "image_df = image_utils.get_image_dimensions(image_files)\n",
    "display(image_df.head())\n",
    "\n",
    "ground_truth_df = ground_truth_df.merge(image_df, on=\"file_number\", how=\"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bioscout_tech_challenge.utils.bounding_box import df_to_bounding_boxes\n",
    "\n",
    "# Now lets turn the ground truth data into a list of bounding boxes\n",
    "ground_truth_boxes = df_to_bounding_boxes(ground_truth_df, method=\"absolute\",file_name=\"file_number\")\n",
    "model_output_boxes = df_to_bounding_boxes(model_output_df, method=\"centroid\",file_name=\"file_number\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ground_truth_boxes[0])\n",
    "display(model_output_boxes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now how two lists of bounding boxes but no easy way to compare them. We need to find the IoU scores for the boxes in the two lists. But only between the boxes that are from the same image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into two new dataframes using the dataclass bounding box\n",
    "from dataclasses import asdict\n",
    "ground_box_df = pd.DataFrame.from_records([asdict(box) for box in ground_truth_boxes])\n",
    "model_box_df = pd.DataFrame.from_records([asdict(box) for box in model_output_boxes])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is more efficent to calculate the statistics using bounding box broken into dataframes. However, we have already created the tools to calculate the statistics using the bounding box dataclass. We will add the list of bounding boxes to our image dataframe and then calculate the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_list = ground_truth_df[\"file_number\"].unique()\n",
    "\n",
    "\n",
    "# Split ground_truth_boxes into sublists\n",
    "start = 0\n",
    "ground_truth_boxes_split = []\n",
    "model_output_boxes_split = []\n",
    "counts = ground_truth_df[\"file_number\"].value_counts().sort_index(key=lambda x: x.astype(int))\n",
    "for count in counts.tolist():\n",
    "    end = start + count\n",
    "    ground_truth_boxes_split.append(ground_truth_boxes[start:end])\n",
    "    start = end\n",
    "start = 0\n",
    "counts = model_output_df[\"file_number\"].value_counts().sort_index(key=lambda x: x.astype(int))\n",
    "for count in counts.tolist():\n",
    "    end = start + count\n",
    "    model_output_boxes_split.append(model_output_boxes[start:end])\n",
    "    start = end\n",
    "\n",
    "\n",
    "# # Add the lists of bounding boxes to the image dataframe\n",
    "image_df['ground_truth_boxes'] = ground_truth_boxes_split\n",
    "image_df['model_output_boxes'] = model_output_boxes_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above was not a great way to process the data as there are no sanity checks that only bounding boxs from the same image are being added to the dataframe. We can check this by looking at the bounding boxes for a single image. But ideally we would process the data in a way that ensures this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 17\n",
    "name = image_df[\"file_number\"].iloc[idx]\n",
    "print(all([box.name==name for box in image_df[\"model_output_boxes\"].iloc[idx]]))\n",
    "print(all([box.name==name for box in image_df[\"ground_truth_boxes\"].iloc[idx]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the metrics for all the images\n",
    "Lets use our metrics functions to calculate the positives negatives, precision, recall and F1 score for each image. We will assume an IoU score of 0.5 to determine a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bioscout_tech_challenge.imagery.metrics import calculate_metrics_for_predictions\n",
    "# Calculate metrics for each row and expand the dictionary into new columns directly\n",
    "metrics_df = pd.DataFrame(\n",
    "    image_df.apply(\n",
    "        lambda row: calculate_metrics_for_predictions(\n",
    "            row['model_output_boxes'],\n",
    "            row['ground_truth_boxes'],\n",
    "            iou_threshold=0.5\n",
    "        ),\n",
    "        axis=1\n",
    "    ).tolist()\n",
    ")\n",
    "\n",
    "# Add metrics columns to original dataframe\n",
    "processed_df = pd.concat([image_df, metrics_df], axis=1)\n",
    "processed_df.drop(columns=[\"model_output_boxes\", \"ground_truth_boxes\"], inplace=True)\n",
    "\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do a sanity check with our original test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = test_file.split(\".\")[0]\n",
    "idx = processed_df[processed_df[\"file_number\"]==file_name].index[0]\n",
    "display(processed_df.iloc[idx])\n",
    "\n",
    "print(\"precision:\", precision)\n",
    "print(\"recall:\", recall) \n",
    "print(\"f1_score:\", f1)\n",
    "print(\"true_positives:\", true_positives)\n",
    "print(\"false_positives:\", false_positives)\n",
    "print(\"false_negatives:\", false_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse low scores\n",
    "Everything matches up. Lets pick some of the images with lower precision, recall and F1 score and see if we can understand why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_image_with_boxes(image_df, idx):\n",
    "    file_name = image_df.loc[idx][\"file_number\"] + \".jpg\"\n",
    "    image = display_image(file_name)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box in image_df.loc[idx][\"ground_truth_boxes\"]:\n",
    "        draw.rectangle(box.to_absolute_coordinates(image.width, image.height), outline=\"red\", width=3)\n",
    "    for box in image_df.loc[idx][\"model_output_boxes\"]:\n",
    "        draw.rectangle(box.to_absolute_coordinates(image.width, image.height), outline=\"blue\", width=3)\n",
    "    image.show()\n",
    "\n",
    "\n",
    "precision_min = processed_df[\"precision\"].idxmin()\n",
    "recall_min = processed_df[\"recall\"].idxmin()\n",
    "f1_min = processed_df[\"f1_score\"].idxmin()\n",
    "display(f\"Precision Minimum idx: {precision_min}\")\n",
    "display(f\"Recall Minimum idx: {recall_min}\")\n",
    "display(f\"F1 Minimum idx: {f1_min}\")\n",
    "\n",
    "display(processed_df.loc[precision_min])\n",
    "display_image_with_boxes(image_df, precision_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly the one image has the lowest precision, recall and F1 score. Whilst it did succesfully identify quite a few of the diesease areas the image contains a large number of false positives as well as a large number of false negatives. The image itself is quite busy and contains a large number of diesease areas. It also seems that most of the false positives are in fact true positives and the ground truth data has not labelled all of the diesease areas. This is the seconnd image we have visualised and as we have seen unlabelled diesease areas it is a cause for concern and will be investigated further.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highest scoring images\n",
    "Whilst the lowest scoring images are the cause for concern we can analyse high scoring images to see where the model is performing well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_max = processed_df[\"precision\"].idxmax()\n",
    "recall_max = processed_df[\"recall\"].idxmax()\n",
    "f1_max = processed_df[\"f1_score\"].idxmax()\n",
    "display(f\"Precision Maximum idx: {precision_max}\")\n",
    "display(f\"Recall Maximum idx: {recall_max}\")\n",
    "display(f\"F1 Maximum idx: {f1_max}\")\n",
    "\n",
    "display(processed_df.loc[precision_max])\n",
    "display_image_with_boxes(image_df, precision_max)\n",
    "\n",
    "display(processed_df.loc[recall_max])\n",
    "display_image_with_boxes(image_df, recall_max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly the image with the highest precision, recall and F1 score has no false positives or negatives. The image itself is quite sparce and the ground truth data has labelled all of the diesease areas. The model has also identified all of the diesease areas. A second image with a perfect recall but containing some false positives again looks like the model has performed perfectly but there has been issues with the labelling of the ground truth data.\n",
    "\n",
    "Lets have a look at the image with the highest f1 score that has both false positives and negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_max = processed_df[(processed_df[\"precision\"]<1) & (processed_df[\"recall\"]<1)][\"f1_score\"].idxmax()\n",
    "display(f\"F1 Maximum idx: {f1_max}\")\n",
    "display(processed_df.loc[f1_max])\n",
    "display_image_with_boxes(image_df, f1_max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting as the false negative seems to be due to the opacity of the diesease area. WHilst the false positive appears to have detected a diesease area that is similar to what we are looking for but it is unclear if it is the same type of diesease. We will need to discuss this with a biologist to determine if the false positive is a true positive due to again the issue of unlabelled diesease areas in the ground truth data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the metrics for all the images\n",
    "\n",
    "Since we have all the metrics in a dataframe lets calculate the precision, recall and F1 score for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up all true positives, false positives, and false negatives across all images\n",
    "total_true_positives = processed_df['true_positives'].sum()\n",
    "total_false_positives = processed_df['false_positives'].sum() \n",
    "total_false_negatives = processed_df['false_negatives'].sum()\n",
    "# Calculate overall metrics\n",
    "overall_precision = total_true_positives / (total_true_positives + total_false_positives)\n",
    "overall_recall = total_true_positives / (total_true_positives + total_false_negatives)\n",
    "overall_f1 = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall)\n",
    "\n",
    "print(f\"Overall Precision: {overall_precision:.3f}\")\n",
    "print(f\"Overall Recall: {overall_recall:.3f}\") \n",
    "print(f\"Overall F1 Score: {overall_f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent the overall precision, recall and F1 score are all relatively high. This is a good indication that the model is performing well across the dataset. It is likely that the precision score is higher due to the issue of unlabelled diesease areas in the ground truth data.\n",
    "\n",
    "## Conclusion and Recommendations\n",
    "\n",
    "The model is performing well across the dataset. However, there are some issues with the ground truth data labelling that need to be addressed. The unlabelled diesease areas are a cause for concern and will need to be investigated further. This is causing a misleading precision score as the are multiple cases of true positive being identified as false positives. This would likely greatly improve the precision score and by consequence the F1 score.\n",
    "\n",
    "Next steps would be to integrate this analysis as a tool in the package. This could be included as a function in the `imagery` module. It could be used to analyse the performance of the model on a single image or across the entire dataset. It could also be used to plot the IoU scores on the image data. To help with the identification of unlabelled diesease areas in the ground truth data a function needs to be added that draws the false positive bounding boxes on the image data and exports so that they can be reviewed by a biologist. This would allow for inveitigation into the issue without needing to rereviewing all the images and diesease areas.\n",
    "\n",
    "Additionally the way in which the data was processed from dataframe to bounding box back to dataframe is not robust and raises issues with data integrity. The process could be made more efficent by processing all the bounding boxes within a dataframe. This would also allow us to keep track of model metadata specifically confidence scores. This is necessary in order to calculate the AP score (average precision) given different confidence thresholds. Which in turn would allow us to calcaulte the mAP (mean average precision) score to give a single value to describe the performance of the model across the dataset.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
