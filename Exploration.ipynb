{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1  \n",
    "# Automatically reload bioscout package\n",
    "%aimport bioscout_tech_challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8adcd618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bioscout_tech_challenge import *\n",
    "# Now any changes to your package will be automatically reloaded\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88680ff1",
   "metadata": {},
   "source": [
    "## Pre-Process Weather Data\n",
    "\n",
    "Things to look at \n",
    "- extra_information column\n",
    "- storage \n",
    "- autodetect header from mutliple files\n",
    "- extract out of sensors to other tables\n",
    "- add new data to existing tables\n",
    "- write to sql db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e068f5",
   "metadata": {},
   "source": [
    "### Merge Weather and Devices Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2de255",
   "metadata": {},
   "source": [
    "Hardcode some data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76efb56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_folder = r\"data/tables/weather_data/\"\n",
    "devices_fn = \"weather_devices.csv\"\n",
    "weather_fn = \"weather_data_1.csv\"\n",
    "\n",
    "\n",
    "devices_df = read_csv_file(weather_folder+devices_fn)\n",
    "weather_df = read_csv_file(weather_folder+weather_fn)\n",
    "\n",
    "print(devices_df.columns)\n",
    "print(weather_df.columns)\n",
    "print(weather_df.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60fb340",
   "metadata": {},
   "source": [
    "Try merging the data check resulting shape makes sense. ie merge_rows = weather_rows and merge columns = weather_columns + device_columns -1 (device_id repeated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0eae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merge_weather_data(weather_df,devices_df)\n",
    "#Check shape makes sense\n",
    "print(devices_df.shape)\n",
    "print(weather_df.shape)\n",
    "print(merged_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96a9e8b",
   "metadata": {},
   "source": [
    "Need to check if any rows are missing corresponding device information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ed6ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any rows are missing corresponding device information \n",
    "# with function get_na_rows pick a random column from devices_df.\n",
    "print(get_na_rows(merged_df,\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f03561",
   "metadata": {},
   "source": [
    "### Find and join multiple Weather Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd3c6a8",
   "metadata": {},
   "source": [
    "Lets try to find multiple files  based on a pattern matching and determine if they all contain header data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9e5982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all csv files in the weather folder\n",
    "weather_csvs = find_csv_files(weather_folder,prefix=\"weather_data\")\n",
    "print(weather_csvs)\n",
    "\n",
    "# Identify the header for each file\n",
    "for csv in weather_csvs:\n",
    "    print(identify_header(csv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8d268",
   "metadata": {},
   "source": [
    "Looks like only the first file has a header. Lets try to combine the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2573a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combine_csv_files(weather_csvs,detect_header=True)\n",
    "print(combined_df.shape)\n",
    "print(combined_df.columns)\n",
    "print(combined_df.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f0aee",
   "metadata": {},
   "source": [
    "Now lets try to merge the combined data with the devices data and check if any rows are missing corresponding device information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2193b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_merged_df = merge_weather_data(combined_df,devices_df)\n",
    "#Check shape makes sense\n",
    "print(combined_merged_df.shape)\n",
    "print(get_na_rows(merged_df,\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689f5845",
   "metadata": {},
   "source": [
    "No issues with the data given but is a good check for integration into the end user application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b42b9d",
   "metadata": {},
   "source": [
    "### Parse Extra Information (Flatten extra_information data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a922109f",
   "metadata": {},
   "source": [
    "Additional sensor readings are stored in the extra_information column as a json string.\n",
    "We need to parse the json string and flatten the data into a table.\n",
    "Start by looking at the data in a sample row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4032bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 500\n",
    "# First get a sample row's extra_information\n",
    "extra_info = combined_merged_df.loc[index]['extra_information']\n",
    "print(extra_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af05227a",
   "metadata": {},
   "source": [
    "String dump of a dictionary stored in json format. Lets make this pretty so we can see the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f8fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the extra_information JSON string to a dictionary\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "# The string appears to be double-encoded (both JSON and string literal), so we need to:\n",
    "# 1. Parse the outer JSON\n",
    "# 2. Evaluate the inner string literal as a Python dict\n",
    "def parse_extra_info(json_str):\n",
    "    # First, clean up the string if needed\n",
    "    cleaned_str = json_str.replace(\"'\", '\"')  # Replace single quotes with double quotes\n",
    "    # Parse JSON\n",
    "    return json.loads(cleaned_str)\n",
    "\n",
    "sample_parsed = parse_extra_info(extra_info)\n",
    "# Create a PrettyPrinter instance with desired formatting\n",
    "pp = pprint.PrettyPrinter(indent=4, width=80)\n",
    "print(\"\\nParsed Extra Information:\")\n",
    "pp.pprint(sample_parsed)\n",
    "\n",
    "# lets also compare to the rest of the table\n",
    "columns_to_print = combined_merged_df.columns.difference(['extra_information'])\n",
    "print(\"\\nRest of Table:\")\n",
    "print(combined_merged_df[columns_to_print].loc[index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69c2582",
   "metadata": {},
   "source": [
    "### Data Inconsistencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0073534b",
   "metadata": {},
   "source": [
    "Mostly contains sensor readings with a sensor type, device name and reading value. However, there are some other entries that are not sensor readings; Timestamp and IotID. Also pressure and VOC are included again despite aleady having a column in the main table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f64e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check of parsed data with existing pressure and VOC readings.\n",
    "print(\"Parsed Data:\")\n",
    "print(f\"Pressure: {sample_parsed['Pressure'][0]['Value']}\")\n",
    "print(f\"VOCs: {sample_parsed['VOCs'][0]['Value']}\")\n",
    "\n",
    "print(\"\\nDataframe Data:\")\n",
    "print(f\"Pressure: {combined_merged_df.loc[index]['pressure']}\")\n",
    "print(f\"VOCs: {combined_merged_df.loc[index]['voc']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41658caf",
   "metadata": {},
   "source": [
    "Tested the above with a few more rows and it seems that pressure is a factor of 10 different and VOCs are a factor of 1000 different. This seems to be consistent based on about 10 checks but we should probably automate this checking when flattening the data.\n",
    "\n",
    "Its unclear how the data was collected and what algorithm was used to calculate the sensor readings. However, the earths pressure is between 900- 1100hPa so the json data seems to be in the correct units.\n",
    "\n",
    "Best solution is probably to add a new column with the units of the sensor reading.\n",
    "\n",
    "Its unclear how the VOC data was transformed. BME680 documentation suggests that it is standard to convert the resistance measured into a IAQ (Indoor Air Quality) reading. However, the key word indoor makes it seem like this might not be the case considering the data is collected outside. \n",
    "\n",
    "Assume the values are converted to the IAQ scale a VOC reading of 500 is considered hazardous and therefore a value of ~10000 is not plausible given the nature of the measurements. However, a reading between 0-50 is excellent air quality which seems likely to be found outdoors on a farm. Therefore, moving forward with the assumption that the json data is in the units of the IAQ scale. (Ideally this would be confirmed with the engineering team)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf4334b",
   "metadata": {},
   "source": [
    "#### Flatten Extra Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d7d04",
   "metadata": {},
   "source": [
    "Moving forward we need to flatten each sensor reading into a new row.\n",
    "\n",
    "Therefore we aim to add the following columns to the dataframe:\n",
    "- sensor_type (name of the measurement ie pressure, voc)\n",
    "- sensor_device (name of the device the measurement was taken from)\n",
    "- sensor_reading (value of the measurement)\n",
    "- sensor_units (units of the measurement)\n",
    "\n",
    "whilst dropping the following columns:\n",
    "- extra_information (data has all been flattened)\n",
    "- pressure (will be replaced with sensor_type and sensor_reading)\n",
    "- voc (will be replaced with sensor_type and sensor_reading)\n",
    "\n",
    "Additionally the IotID is new unique information and will be copied over each new expanded row.\n",
    "\n",
    "The timestamp data seems to be consistent with the main dataframe however it would be nice to do a sanity check and flagging any inconsistencies. A simple solution based on the above analysis is to assume the json data is correct and should be the source of truth. Therefore, any timestamp in the main dataframe that disagrees with the timestamp in the json data should be flagged as an inconsistency.\n",
    "\n",
    "\n",
    "Finally a design decision needs to be made about the sensors that have a SampleTimeLength. Whilst the rest of the sensors are assuming to sample a discrete point in time, the rain sensors are collected over a short period.\n",
    "\n",
    "One solution would be to make a sample time length column and fill it with -1 for sensors that do not have a sample time length. This is probably the simplest solution but may not be the best from a data storage perspective. \n",
    "\n",
    "Without knowing the direction of the data in the future this is probably the best solution without introducing my own assumptions unneccesarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b52e11c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lets make a sensor schema to help with the parsing\n",
    "# This is a placeholder for now and will be updated as we learn more about the data.\n",
    "# The keys are the sensor types and the values are a list of the sensor type, units.\n",
    "sensor_schema = {\n",
    "    'Humidity': ['humidity', '%'],\n",
    "    'Pressure': ['pressure', 'hPa'],\n",
    "    'Rainfall': ['rainfall', 'mm'],\n",
    "    'Temperature': ['temperature', 'C'],\n",
    "    'VOCs': ['voc', 'IAQ'],\n",
    "    'WindDirection': ['winddirection', 'degrees'],\n",
    "    'WindSpeed': ['windspeed', 'm/s'],\n",
    "}\n",
    "\n",
    "expanded_df = expand_extra_information(combined_merged_df.loc[index],sensor_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff05630",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expanded_df.columns)\n",
    "print(expanded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3178808b",
   "metadata": {},
   "source": [
    "Looks pretty good. Now we need to apply this to the whole dataframe and add the functionality to the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4f8a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    flattened_df\n",
    "except NameError:\n",
    "    flattened_df = expand_weather_dataframe(combined_merged_df[0:100],sensor_schema)\n",
    "print(flattened_df.columns)\n",
    "print(flattened_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac6c458",
   "metadata": {},
   "source": [
    "This approach is useful for a small dataframe but is not scalable. We need to find a way to vectorize the process. A quick search suggest that the native python json library is particularly slow and is not suitable for large dataframes.\n",
    "\n",
    "Lets refactor the code and use Pandas json_normalize function. Since this task is not dependent on the device information we can create a function that only deals with the extra_information column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ae7d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_weather_df = flatten_weather_data(combined_df)\n",
    "print(flattened_weather_df.columns)\n",
    "print(flattened_weather_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e33771",
   "metadata": {},
   "source": [
    "Also need to add the sensor units to the dataframe and check the timestamp match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a7cfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_units = {\n",
    "    'humidity': '%',\n",
    "    'pressure': 'hPa',\n",
    "    'rainfall': 'mm',\n",
    "    'temperature': 'C',\n",
    "    'voc': 'IAQ',\n",
    "    'winddirection': 'degrees',\n",
    "    'windspeed': 'm/s',\n",
    "}\n",
    "\n",
    "flattened_weather_df = add_sensor_units(flattened_weather_df,sensor_units)\n",
    "print(check_timestamp_match(flattened_weather_df))\n",
    "if len(check_timestamp_match(flattened_weather_df)) > 0:\n",
    "    print(\"Warning: Timestamps do not match\")\n",
    "    print(flattened_weather_df.loc[check_timestamp_match(flattened_weather_df)])\n",
    "else:\n",
    "    flattened_weather_df.drop(columns=['date_measured'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab21dcde",
   "metadata": {},
   "source": [
    "Need to refactor the changes to the device information table into a separate function for adding a timezone and checking the timestamp. This can be seperate function since it is not dependent on the weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "devices_timezone_df = add_timezone_from_coordinates(devices_df)\n",
    "print(devices_timezone_df.columns)\n",
    "print(devices_timezone_df.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a15c868",
   "metadata": {},
   "source": [
    "Alright now lets merge the weather and devices dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expanded_df.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87faf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = merge_weather_data(flattened_weather_df,devices_timezone_df)\n",
    "print(final_df.columns)\n",
    "final_row = final_df.loc[0]\n",
    "print(final_row)\n",
    "#compare to original combined_merged_df\n",
    "flattened_row  = flattened_df.loc[0]  \n",
    "print(flattened_row)\n",
    "# find the difference in columns\n",
    "print(final_df.columns.difference(flattened_df.columns))\n",
    "print(flattened_df.columns.difference(final_df.columns))\n",
    "# Find columns that are in both dataframes but have different names\n",
    "common_values = set(final_df.columns) & set(flattened_df.columns)\n",
    "print(\"\\nCommon columns:\", common_values)\n",
    "print(\"Match:\", (final_row[list(common_values)] == flattened_row[list(common_values)]).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b4d438",
   "metadata": {},
   "source": [
    "Great that works and is much faster. Need to now add the functionality to the package for the end user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671fd3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try and load the sensor schema from a json file.\n",
    "sensor_schema = read_json_file(r\"src/bioscout_tech_challenge/sensor_schema.json\")\n",
    "print(parse_sensor_schema(sensor_schema))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20422796",
   "metadata": {},
   "source": [
    "CLI is working as expected lets merge and save the data through the cli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f256f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! (bioscout-tech-challenge flatten -c \\\n",
    "    --directory=data/tables/weather_data/ \\\n",
    "    --output=data/tables/weather_data/output/exploration_flattened_output.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "! ( bioscout-tech-challenge merge -tz\\\n",
    "    --device_csv=data/tables/weather_data/weather_devices.csv \\\n",
    "    --weather_csv=data/tables/weather_data/output/exploration_flattened_output.csv \\\n",
    "    --output=data/tables/weather_data/output/exploration_merged_output.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07f69ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f df\n",
    "%reset Out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
