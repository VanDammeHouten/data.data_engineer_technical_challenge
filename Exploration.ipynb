{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1  \n",
    "# Automatically reload bioscout package\n",
    "%aimport bioscout_tech_challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8adcd618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bioscout_tech_challenge import *\n",
    "from bioscout_tech_challenge.utils.weather import *\n",
    "from bioscout_tech_challenge.utils.file_operations import *\n",
    "# Now any changes to your package will be automatically reloaded\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88680ff1",
   "metadata": {},
   "source": [
    "## Pre-Process Weather Data\n",
    "\n",
    "Things to look at \n",
    "- extra_information column\n",
    "- storage \n",
    "- autodetect header from mutliple files\n",
    "- extract out of sensors to other tables\n",
    "- add new data to existing tables\n",
    "- write to sql db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e068f5",
   "metadata": {},
   "source": [
    "### Merge Weather and Devices Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2de255",
   "metadata": {},
   "source": [
    "Hardcode some data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76efb56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['device_id', 'device_name', 'site_id', 'utc_offset_in_hours',\n",
      "       'longitude', 'latitude'],\n",
      "      dtype='object')\n",
      "Index(['index', 'weather_reading_id', 'date_measured', 'device_id', 'voc',\n",
      "       'pressure', 'extra_information'],\n",
      "      dtype='object')\n",
      "index                                                           7243411\n",
      "weather_reading_id                                              9983574\n",
      "date_measured                                 2024-10-19 11:56:31+00:00\n",
      "device_id                                                           259\n",
      "voc                                                              9400.0\n",
      "pressure                                                        10246.0\n",
      "extra_information     {'VOCs': [{'Value': 9.4, 'Sensor': 'BME680'}],...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "weather_folder = r\"data/tables/weather_data/\"\n",
    "devices_fn = \"weather_devices.csv\"\n",
    "weather_fn = \"weather_data_1.csv\"\n",
    "\n",
    "\n",
    "devices_df = read_csv_file(weather_folder+devices_fn)\n",
    "weather_df = read_csv_file(weather_folder+weather_fn)\n",
    "\n",
    "print(devices_df.columns)\n",
    "print(weather_df.columns)\n",
    "print(weather_df.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60fb340",
   "metadata": {},
   "source": [
    "Try merging the data check resulting shape makes sense. ie merge_rows = weather_rows and merge columns = weather_columns + device_columns -1 (device_id repeated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f0eae25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 6)\n",
      "(49999, 7)\n",
      "(49999, 12)\n"
     ]
    }
   ],
   "source": [
    "merged_df = merge_weather_data(weather_df,devices_df)\n",
    "#Check shape makes sense\n",
    "print(devices_df.shape)\n",
    "print(weather_df.shape)\n",
    "print(merged_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96a9e8b",
   "metadata": {},
   "source": [
    "Need to check if any rows are missing corresponding device information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53ed6ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [index, device_id]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check if any rows are missing corresponding device information \n",
    "# with function get_na_rows pick a random column from devices_df.\n",
    "print(get_na_rows(merged_df,\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f03561",
   "metadata": {},
   "source": [
    "### Find and join multiple Weather Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd3c6a8",
   "metadata": {},
   "source": [
    "Lets try to find multiple files  based on a pattern matching and determine if they all contain header data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec9e5982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/home/zach/repo/data.data_engineer_technical_challenge/data/tables/weather_data/weather_data_1.csv'), PosixPath('/home/zach/repo/data.data_engineer_technical_challenge/data/tables/weather_data/weather_data_2.csv'), PosixPath('/home/zach/repo/data.data_engineer_technical_challenge/data/tables/weather_data/weather_data_3.csv')]\n",
      "infer\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Find all csv files in the weather folder\n",
    "weather_csvs = find_csv_files(weather_folder,prefix=\"weather_data\")\n",
    "print(weather_csvs)\n",
    "\n",
    "# Identify the header for each file\n",
    "for csv in weather_csvs:\n",
    "    print(identify_header(csv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8d268",
   "metadata": {},
   "source": [
    "Looks like only the first file has a header. Lets try to combine the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2573a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144399, 8)\n",
      "Index(['index', 'weather_reading_id', 'date_measured', 'device_id', 'voc',\n",
      "       'pressure', 'extra_information', 'source_file'],\n",
      "      dtype='object')\n",
      "index                                                           7243411\n",
      "weather_reading_id                                              9983574\n",
      "date_measured                                 2024-10-19 11:56:31+00:00\n",
      "device_id                                                           259\n",
      "voc                                                              9400.0\n",
      "pressure                                                        10246.0\n",
      "extra_information     {'VOCs': [{'Value': 9.4, 'Sensor': 'BME680'}],...\n",
      "source_file                                          weather_data_1.csv\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "combined_df = combine_csv_files(weather_csvs,detect_header=True)\n",
    "print(combined_df.shape)\n",
    "print(combined_df.columns)\n",
    "print(combined_df.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f0aee",
   "metadata": {},
   "source": [
    "Now lets try to merge the combined data with the devices data and check if any rows are missing corresponding device information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c2193b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144399, 13)\n",
      "Empty DataFrame\n",
      "Columns: [index, device_id]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "combined_merged_df = merge_weather_data(combined_df,devices_df)\n",
    "#Check shape makes sense\n",
    "print(combined_merged_df.shape)\n",
    "print(get_na_rows(merged_df,\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689f5845",
   "metadata": {},
   "source": [
    "No issues with the data given but is a good check for integration into the end user application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b42b9d",
   "metadata": {},
   "source": [
    "### Parse Extra Information (Flatten extra_information data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a922109f",
   "metadata": {},
   "source": [
    "Additional sensor readings are stored in the extra_information column as a json string.\n",
    "We need to parse the json string and flatten the data into a table.\n",
    "Start by looking at the data in a sample row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4032bc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'VOCs': [{'Value': 12.044, 'Sensor': 'BME680'}], 'IotID': '0a10aced202194944a051624', 'Humidity': [{'Value': 56.283, 'Sensor': 'BME680'}, {'Value': 100, 'Sensor': 'SHT30'}], 'Pressure': [{'Value': 985.11, 'Sensor': 'BME680'}], 'Rainfall': [{'Value': 0, 'Sensor': 'OpticalRainGauge', 'SampleTimeLength': 300}, {'Value': 0, 'Sensor': 'TippingRainGauge', 'SampleTimeLength': 300}], 'Timestamp': '2024-10-19T15:22:02Z', 'WindSpeed': [{'Value': 0.856, 'Sensor': '40ms_spin_wind'}, {'Value': 1.092, 'Sensor': '60ms_louvre_us'}], 'Temperature': [{'Value': 5.36, 'Sensor': 'BME680'}, {'Value': 4.27291, 'Sensor': 'SHT30'}], 'WindDirection': [{'Value': 298.039, 'Sensor': '40ms_spin_wind'}, {'Value': 112.795, 'Sensor': '60ms_louvre_us'}]}\n"
     ]
    }
   ],
   "source": [
    "index = 500\n",
    "# First get a sample row's extra_information\n",
    "extra_info = combined_merged_df.loc[index]['extra_information']\n",
    "print(extra_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af05227a",
   "metadata": {},
   "source": [
    "String dump of a dictionary stored in json format. Lets make this pretty so we can see the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae0f8fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsed Extra Information:\n",
      "{   'Humidity': [   {'Sensor': 'BME680', 'Value': 56.283},\n",
      "                    {'Sensor': 'SHT30', 'Value': 100}],\n",
      "    'IotID': '0a10aced202194944a051624',\n",
      "    'Pressure': [{'Sensor': 'BME680', 'Value': 985.11}],\n",
      "    'Rainfall': [   {   'SampleTimeLength': 300,\n",
      "                        'Sensor': 'OpticalRainGauge',\n",
      "                        'Value': 0},\n",
      "                    {   'SampleTimeLength': 300,\n",
      "                        'Sensor': 'TippingRainGauge',\n",
      "                        'Value': 0}],\n",
      "    'Temperature': [   {'Sensor': 'BME680', 'Value': 5.36},\n",
      "                       {'Sensor': 'SHT30', 'Value': 4.27291}],\n",
      "    'Timestamp': '2024-10-19T15:22:02Z',\n",
      "    'VOCs': [{'Sensor': 'BME680', 'Value': 12.044}],\n",
      "    'WindDirection': [   {'Sensor': '40ms_spin_wind', 'Value': 298.039},\n",
      "                         {'Sensor': '60ms_louvre_us', 'Value': 112.795}],\n",
      "    'WindSpeed': [   {'Sensor': '40ms_spin_wind', 'Value': 0.856},\n",
      "                     {'Sensor': '60ms_louvre_us', 'Value': 1.092}]}\n",
      "\n",
      "Rest of Table:\n",
      "date_measured          2024-10-19 15:22:02+00:00\n",
      "device_id                                    262\n",
      "device_name                              WH-0010\n",
      "index                                    7365694\n",
      "latitude                              -43.418845\n",
      "longitude                             171.388011\n",
      "pressure                                  9851.1\n",
      "site_id                                       57\n",
      "source_file                   weather_data_1.csv\n",
      "utc_offset_in_hours                         13.0\n",
      "voc                                      12044.0\n",
      "weather_reading_id                       9987227\n",
      "Name: 500, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert the extra_information JSON string to a dictionary\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "# The string appears to be double-encoded (both JSON and string literal), so we need to:\n",
    "# 1. Parse the outer JSON\n",
    "# 2. Evaluate the inner string literal as a Python dict\n",
    "def parse_extra_info(json_str):\n",
    "    # First, clean up the string if needed\n",
    "    cleaned_str = json_str.replace(\"'\", '\"')  # Replace single quotes with double quotes\n",
    "    # Parse JSON\n",
    "    return json.loads(cleaned_str)\n",
    "\n",
    "sample_parsed = parse_extra_info(extra_info)\n",
    "# Create a PrettyPrinter instance with desired formatting\n",
    "pp = pprint.PrettyPrinter(indent=4, width=80)\n",
    "print(\"\\nParsed Extra Information:\")\n",
    "pp.pprint(sample_parsed)\n",
    "\n",
    "# lets also compare to the rest of the table\n",
    "columns_to_print = combined_merged_df.columns.difference(['extra_information'])\n",
    "print(\"\\nRest of Table:\")\n",
    "print(combined_merged_df[columns_to_print].loc[index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69c2582",
   "metadata": {},
   "source": [
    "### Data Inconsistencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0073534b",
   "metadata": {},
   "source": [
    "Mostly contains sensor readings with a sensor type, device name and reading value. However, there are some other entries that are not sensor readings; Timestamp and IotID. Also pressure and VOC are included again despite aleady having a column in the main table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f64e377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed Data:\n",
      "Pressure: 985.11\n",
      "VOCs: 12.044\n",
      "\n",
      "Dataframe Data:\n",
      "Pressure: 9851.1\n",
      "VOCs: 12044.0\n"
     ]
    }
   ],
   "source": [
    "# Sanity check of parsed data with existing pressure and VOC readings.\n",
    "print(\"Parsed Data:\")\n",
    "print(f\"Pressure: {sample_parsed['Pressure'][0]['Value']}\")\n",
    "print(f\"VOCs: {sample_parsed['VOCs'][0]['Value']}\")\n",
    "\n",
    "print(\"\\nDataframe Data:\")\n",
    "print(f\"Pressure: {combined_merged_df.loc[index]['pressure']}\")\n",
    "print(f\"VOCs: {combined_merged_df.loc[index]['voc']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41658caf",
   "metadata": {},
   "source": [
    "Tested the above with a few more rows and it seems that pressure is a factor of 10 different and VOCs are a factor of 1000 different. This seems to be consistent based on about 10 checks but we should probably automate this checking when flattening the data.\n",
    "\n",
    "Its unclear how the data was collected and what algorithm was used to calculate the sensor readings. However, the earths pressure is between 900- 1100hPa so the json data seems to be in the correct units.\n",
    "\n",
    "Best solution is probably to add a new column with the units of the sensor reading.\n",
    "\n",
    "Its unclear how the VOC data was transformed. BME680 documentation suggests that it is standard to convert the resistance measured into a IAQ (Indoor Air Quality) reading. However, the key word indoor makes it seem like this might not be the case considering the data is collected outside. \n",
    "\n",
    "Assume the values are converted to the IAQ scale a VOC reading of 500 is considered hazardous and therefore a value of ~10000 is not plausible given the nature of the measurements. However, a reading between 0-50 is excellent air quality which seems likely to be found outdoors on a farm. Therefore, moving forward with the assumption that the json data is in the units of the IAQ scale. (Ideally this would be confirmed with the engineering team)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf4334b",
   "metadata": {},
   "source": [
    "#### Flatten Extra Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d7d04",
   "metadata": {},
   "source": [
    "Moving forward we need to flatten each sensor reading into a new row.\n",
    "\n",
    "Therefore we aim to add the following columns to the dataframe:\n",
    "- sensor_type (name of the measurement ie pressure, voc)\n",
    "- sensor_device (name of the device the measurement was taken from)\n",
    "- sensor_reading (value of the measurement)\n",
    "- sensor_units (units of the measurement)\n",
    "\n",
    "whilst dropping the following columns:\n",
    "- extra_information (data has all been flattened)\n",
    "- pressure (will be replaced with sensor_type and sensor_reading)\n",
    "- voc (will be replaced with sensor_type and sensor_reading)\n",
    "\n",
    "Additionally the IotID is new unique information and will be copied over each new expanded row.\n",
    "\n",
    "The timestamp data seems to be consistent with the main dataframe however it would be nice to do a sanity check and flagging any inconsistencies. A simple solution based on the above analysis is to assume the json data is correct and should be the source of truth. Therefore, any timestamp in the main dataframe that disagrees with the timestamp in the json data should be flagged as an inconsistency.\n",
    "\n",
    "\n",
    "Finally a design decision needs to be made about the sensors that have a SampleTimeLength. Whilst the rest of the sensors are assuming to sample a discrete point in time, the rain sensors are collected over a short period.\n",
    "\n",
    "One solution would be to make a sample time length column and fill it with -1 for sensors that do not have a sample time length. This is probably the simplest solution but may not be the best from a data storage perspective. \n",
    "\n",
    "Without knowing the direction of the data in the future this is probably the best solution without introducing my own assumptions unneccesarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b52e11c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/micromamba/envs/bioscout/lib/python3.13/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'timestamp'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# lets make a sensor schema to help with the parsing\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This is a placeholder for now and will be updated as we learn more about the data.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# The keys are the sensor types and the values are a list of the sensor type, units.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m sensor_schema \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHumidity\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhumidity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPressure\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpressure\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhPa\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWindSpeed\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindspeed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm/s\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     12\u001b[0m }\n\u001b[0;32m---> 14\u001b[0m expanded_df \u001b[38;5;241m=\u001b[39m \u001b[43mexpand_extra_information\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_merged_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43msensor_schema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repo/data.data_engineer_technical_challenge/src/bioscout_tech_challenge/utils/weather.py:132\u001b[0m, in \u001b[0;36mexpand_extra_information\u001b[0;34m(row, sensor_schema, extra_information_column, dropped_columns, unique_columns)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Find the timezone from the UTC offset (Not implemented yet)\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# timezone_utc = get_timezone_from_utc_offset(row['utc_offset_in_hours'])\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m#     raise ValueError(f\"Timezone from coordinates does not match timezone from UTC offset\")\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m    131\u001b[0m row_dropped[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimezone\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m timezone_coord\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrow_dropped\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m    133\u001b[0m     row_dropped[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m row_dropped[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtz_localize(row_dropped[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimezone\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/micromamba/envs/bioscout/lib/python3.13/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/micromamba/envs/bioscout/lib/python3.13/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/micromamba/envs/bioscout/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'timestamp'"
     ]
    }
   ],
   "source": [
    "\n",
    "# lets make a sensor schema to help with the parsing\n",
    "# This is a placeholder for now and will be updated as we learn more about the data.\n",
    "# The keys are the sensor types and the values are a list of the sensor type, units.\n",
    "sensor_schema = {\n",
    "    'Humidity': ['humidity', '%'],\n",
    "    'Pressure': ['pressure', 'hPa'],\n",
    "    'Rainfall': ['rainfall', 'mm'],\n",
    "    'Temperature': ['temperature', 'C'],\n",
    "    'VOCs': ['voc', 'IAQ'],\n",
    "    'WindDirection': ['winddirection', 'degrees'],\n",
    "    'WindSpeed': ['windspeed', 'm/s'],\n",
    "}\n",
    "\n",
    "expanded_df = expand_extra_information(combined_merged_df.loc[index],sensor_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff05630",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expanded_df.columns)\n",
    "print(expanded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3178808b",
   "metadata": {},
   "source": [
    "Looks pretty good. Now we need to apply this to the whole dataframe and add the functionality to the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4f8a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    flattened_df\n",
    "except NameError:\n",
    "    flattened_df = expand_weather_dataframe(combined_merged_df[0:100],sensor_schema)\n",
    "print(flattened_df.columns)\n",
    "print(flattened_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac6c458",
   "metadata": {},
   "source": [
    "This approach is useful for a small dataframe but is not scalable. We need to find a way to vectorize the process. A quick search suggest that the native python json library is particularly slow and is not suitable for large dataframes.\n",
    "\n",
    "Lets refactor the code and use Pandas json_normalize function. Since this task is not dependent on the device information we can create a function that only deals with the extra_information column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ae7d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_weather_df = flatten_weather_data(combined_df)\n",
    "print(flattened_weather_df.columns)\n",
    "print(flattened_weather_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e33771",
   "metadata": {},
   "source": [
    "Also need to add the sensor units to the dataframe and check the timestamp match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a7cfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_units = {\n",
    "    'humidity': '%',\n",
    "    'pressure': 'hPa',\n",
    "    'rainfall': 'mm',\n",
    "    'temperature': 'C',\n",
    "    'voc': 'IAQ',\n",
    "    'winddirection': 'degrees',\n",
    "    'windspeed': 'm/s',\n",
    "}\n",
    "\n",
    "flattened_weather_df = add_sensor_units(flattened_weather_df,sensor_units)\n",
    "print(check_timestamp_match(flattened_weather_df))\n",
    "if len(check_timestamp_match(flattened_weather_df)) > 0:\n",
    "    print(\"Warning: Timestamps do not match\")\n",
    "    print(flattened_weather_df.loc[check_timestamp_match(flattened_weather_df)])\n",
    "else:\n",
    "    flattened_weather_df.drop(columns=['date_measured'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab21dcde",
   "metadata": {},
   "source": [
    "Need to refactor the changes to the device information table into a separate function for adding a timezone and checking the timestamp. This can be seperate function since it is not dependent on the weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "devices_timezone_df = add_timezone_from_coordinates(devices_df)\n",
    "print(devices_timezone_df.columns)\n",
    "print(devices_timezone_df.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a15c868",
   "metadata": {},
   "source": [
    "Alright now lets merge the weather and devices dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expanded_df.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87faf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = merge_weather_data(flattened_weather_df,devices_timezone_df)\n",
    "print(final_df.columns)\n",
    "final_row = final_df.loc[0]\n",
    "print(final_row)\n",
    "#compare to original combined_merged_df\n",
    "flattened_row  = flattened_df.loc[0]  \n",
    "print(flattened_row)\n",
    "# find the difference in columns\n",
    "print(final_df.columns.difference(flattened_df.columns))\n",
    "print(flattened_df.columns.difference(final_df.columns))\n",
    "# Find columns that are in both dataframes but have different names\n",
    "common_values = set(final_df.columns) & set(flattened_df.columns)\n",
    "print(\"\\nCommon columns:\", common_values)\n",
    "print(\"Match:\", (final_row[list(common_values)] == flattened_row[list(common_values)]).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b4d438",
   "metadata": {},
   "source": [
    "Great that works and is much faster. Need to now add the functionality to the package for the end user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671fd3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try and load the sensor schema from a json file.\n",
    "sensor_schema = read_json_file(r\"src/bioscout_tech_challenge/sensor_schema.json\")\n",
    "print(parse_sensor_schema(sensor_schema))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20422796",
   "metadata": {},
   "source": [
    "CLI is working as expected lets merge and save the data through the cli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f256f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! (bioscout-tech-challenge flatten -c \\\n",
    "    --directory=data/tables/weather_data/ \\\n",
    "    --output=data/tables/weather_data/output/exploration_flattened_output.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "! ( bioscout-tech-challenge merge -tz\\\n",
    "    --device_csv=data/tables/weather_data/weather_devices.csv \\\n",
    "    --weather_csv=data/tables/weather_data/output/exploration_flattened_output.csv \\\n",
    "    --output=data/tables/weather_data/output/exploration_merged_output.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07f69ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
